{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41f0b0f-cc96-43a2-ab7f-f3264587e7d2",
   "metadata": {
    "id": "b41f0b0f-cc96-43a2-ab7f-f3264587e7d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fernandokenjisakabe/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from numpy.random import default_rng\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e9560b-749a-49a8-aedf-44a1ac9b4bb1",
   "metadata": {
    "id": "c9e9560b-749a-49a8-aedf-44a1ac9b4bb1"
   },
   "outputs": [],
   "source": [
    "# Loading pipeline params.\n",
    "DATA_PATH = Path('data/')\n",
    "FILENAME_STOP = 'stop_90_diff_quat.csv'\n",
    "FILENAME_0 = '0_90_diff_quat.csv'\n",
    "FILENAME_90 = '90_90_diff_quat.csv'\n",
    "FILENAME_180 = '180_90_diff_quat.csv'\n",
    "FILENAME_270 = '270_90_diff_quat.csv'\n",
    "\n",
    "# # Loading pipeline params.\n",
    "# DATA_PATH = Path('data')\n",
    "# FILENAME_STOP = 'stop.csv'\n",
    "# FILENAME_0 = 'm_0.csv'\n",
    "# FILENAME_90 = 'm_90.csv'\n",
    "# FILENAME_180 = 'm_180.csv'\n",
    "# FILENAME_270 = 'm_270.csv'\n",
    "\n",
    "\n",
    "NUM_TIMESTEPS = 6\n",
    "IGNORE_FRACTION = 0.1\n",
    "TEST_FRACTION = 0.25\n",
    "VALIDATION_FRACTION = 0.25\n",
    "\n",
    "\n",
    "# Training params.\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a045adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    path_stop,\n",
    "    path_0,\n",
    "    path_90,\n",
    "    path_180,\n",
    "    path_270,\n",
    "    num_timesteps,\n",
    "    ignore_fraction,\n",
    "    test_fraction,\n",
    "):\n",
    "\n",
    "    def read_and_prepare_data(\n",
    "        path,\n",
    "        class_,\n",
    "        num_timesteps,\n",
    "        ignore_fraction,\n",
    "        test_fraction,\n",
    "    ):\n",
    "\n",
    "        def split_temporally(X, test_fraction):\n",
    "            num_samples = X.shape[0]\n",
    "            num_train = int(num_samples * (1.0 - test_fraction))\n",
    "            X_train, X_test = X[:num_train], X[num_train:]\n",
    "            return X_train, X_test\n",
    "\n",
    "        def transform_into_sequences(X, num_timesteps):\n",
    "            num_samples = X.shape[0]\n",
    "            X_seq = []\n",
    "            for k in range(num_samples - num_timesteps + 1):\n",
    "                X_seq.append(X[k:(k + num_timesteps)])\n",
    "            X_seq = np.array(X_seq)\n",
    "            y_seq = class_ * np.ones((X_seq.shape[0], 1))\n",
    "            return X_seq, y_seq\n",
    "\n",
    "        X = pd.read_csv(path).values\n",
    "        X = X[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13]]\n",
    "\n",
    "        # Ignore beginning and end.\n",
    "        num_samples = X.shape[0]\n",
    "        num_ignore = int(ignore_fraction * num_samples)\n",
    "        X = X[num_ignore:-num_ignore]\n",
    "\n",
    "        # Split X temporally.\n",
    "        X_train, X_test = split_temporally(X, test_fraction)\n",
    "        \n",
    "        # Transform X into short sequences.\n",
    "        X_train, y_train = transform_into_sequences(X_train, num_timesteps)\n",
    "        X_test, y_test = transform_into_sequences(X_test, num_timesteps)\n",
    "        X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "    def shuffle_data(X, y):\n",
    "        idx = np.arange(len(X))\n",
    "        default_rng().shuffle(idx)\n",
    "        X_shuffled = X[idx, :]\n",
    "        y_shuffled = y[idx]\n",
    "        return X_shuffled, y_shuffled\n",
    "\n",
    "    X_train_stop, y_train_stop, X_test_stop, y_test_stop, X_val_stop, y_val_stop = \\\n",
    "        read_and_prepare_data(\n",
    "            path_stop,\n",
    "            0.0,\n",
    "            num_timesteps,\n",
    "            ignore_fraction,\n",
    "            test_fraction,\n",
    "        )\n",
    "    \n",
    "    X_train_0, y_train_0, X_test_0, y_test_0, X_val_0, y_val_0 = \\\n",
    "        read_and_prepare_data(\n",
    "            path_0,\n",
    "            1.0,\n",
    "            num_timesteps,\n",
    "            ignore_fraction,\n",
    "            test_fraction,\n",
    "        )\n",
    "\n",
    "    X_train_90, y_train_90, X_test_90, y_test_90, X_val_90, y_val_90 = \\\n",
    "        read_and_prepare_data(\n",
    "            path_90,\n",
    "            2.0,\n",
    "            num_timesteps,\n",
    "            ignore_fraction,\n",
    "            test_fraction,\n",
    "        )\n",
    "    \n",
    "    X_train_180, y_train_180, X_test_180, y_test_180, X_val_180, y_val_180 = \\\n",
    "        read_and_prepare_data(\n",
    "            path_180,\n",
    "            3.0,\n",
    "            num_timesteps,\n",
    "            ignore_fraction,\n",
    "            test_fraction,\n",
    "        )\n",
    "    \n",
    "    X_train_270, y_train_270, X_test_270, y_test_270,X_val_270, y_val_270  = \\\n",
    "        read_and_prepare_data(\n",
    "            path_270,\n",
    "            4.0,\n",
    "            num_timesteps,\n",
    "            ignore_fraction,\n",
    "            test_fraction,\n",
    "        )\n",
    "\n",
    "    X_train = np.concatenate((X_train_stop, X_train_0, X_train_90, X_train_180, X_train_270), axis=0)\n",
    "    y_train = np.concatenate((y_train_stop, y_train_0, y_train_90, y_train_180, y_train_270), axis=0)\n",
    "    X_test = np.concatenate((X_test_stop, X_test_0, X_test_90, X_test_180, X_test_270), axis=0)\n",
    "    y_test = np.concatenate((y_test_stop, y_test_0, y_test_90, y_test_180, y_test_270), axis=0)\n",
    "    X_val = np.concatenate((X_val_stop, X_val_0, X_val_90, X_val_180, X_val_270), axis=0)\n",
    "    y_val = np.concatenate((y_val_stop, y_val_0, y_val_90, y_val_180, y_val_270), axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train, y_train = shuffle_data(X_train, y_train)\n",
    "    X_test, y_test = shuffle_data(X_test, y_test)\n",
    "    X_val, y_val = shuffle_data(X_val, y_val)\n",
    "    return X_train, X_test,X_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "path_stop = Path(DATA_PATH, FILENAME_STOP)\n",
    "path_0 = Path(DATA_PATH, FILENAME_0)\n",
    "path_90 = Path(DATA_PATH, FILENAME_90)\n",
    "path_180 = Path(DATA_PATH, FILENAME_180)\n",
    "path_270 = Path(DATA_PATH, FILENAME_270)\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test , y_val= load_data(\n",
    "    path_stop,\n",
    "    path_0,\n",
    "    path_90,\n",
    "    path_180,\n",
    "    path_270,\n",
    "    NUM_TIMESTEPS,\n",
    "    IGNORE_FRACTION,\n",
    "    TEST_FRACTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f52716d-1339-4484-9fd3-d5206979ba8e",
   "metadata": {
    "id": "7f52716d-1339-4484-9fd3-d5206979ba8e"
   },
   "outputs": [],
   "source": [
    "# def load_data(\n",
    "#     path_stop,\n",
    "#     path_0,\n",
    "#     path_90,\n",
    "#     path_180,\n",
    "#     path_270,\n",
    "#     num_timesteps,\n",
    "#     ignore_fraction,\n",
    "#     test_fraction,\n",
    "# ):\n",
    "\n",
    "#     def read_and_prepare_data(\n",
    "#         path,\n",
    "#         class_,\n",
    "#         num_timesteps,\n",
    "#         ignore_fraction,\n",
    "#         test_fraction,\n",
    "#     ):\n",
    "\n",
    "#         def split_temporally(X, test_fraction):\n",
    "#             num_samples = X.shape[0]\n",
    "#             num_train = int(num_samples * (1.0 - test_fraction))\n",
    "#             X_train, X_test = X[:num_train], X[num_train:]\n",
    "#             return X_train, X_test\n",
    "\n",
    "#         def transform_into_sequences(X, num_timesteps):\n",
    "#             num_samples = X.shape[0]\n",
    "#             X_seq = []\n",
    "#             for k in range(num_samples - num_timesteps + 1):\n",
    "#                 X_seq.append(X[k:(k + num_timesteps)])\n",
    "#             X_seq = np.array(X_seq)\n",
    "#             y_seq = class_ * np.ones((X_seq.shape[0], 1))\n",
    "#             return X_seq, y_seq\n",
    "\n",
    "#         X = pd.read_csv(path).values\n",
    "#         X = X[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13]]\n",
    "\n",
    "#         # Ignore beginning and end.\n",
    "#         num_samples = X.shape[0]\n",
    "#         num_ignore = int(ignore_fraction * num_samples)\n",
    "#         X = X[num_ignore:-num_ignore]\n",
    "\n",
    "#         # Split X temporally.\n",
    "#         X_train, X_test = split_temporally(X, test_fraction)\n",
    "\n",
    "#         # Transform X into short sequences.\n",
    "#         X_train, y_train = transform_into_sequences(X_train, num_timesteps)\n",
    "#         X_test, y_test = transform_into_sequences(X_test, num_timesteps)\n",
    "\n",
    "#         return X_train, y_train, X_test, y_test\n",
    "\n",
    "#     def shuffle_data(X, y):\n",
    "#         idx = np.arange(len(X))\n",
    "#         default_rng().shuffle(idx)\n",
    "#         X_shuffled = X[idx, :]\n",
    "#         y_shuffled = y[idx]\n",
    "#         return X_shuffled, y_shuffled\n",
    "\n",
    "#     X_train_stop, y_train_stop, X_test_stop, y_test_stop = \\\n",
    "#         read_and_prepare_data(\n",
    "#             path_stop,\n",
    "#             0.0,\n",
    "#             num_timesteps,\n",
    "#             ignore_fraction,\n",
    "#             test_fraction,\n",
    "#         )\n",
    "    \n",
    "#     X_train_0, y_train_0, X_test_0, y_test_0 = \\\n",
    "#         read_and_prepare_data(\n",
    "#             path_0,\n",
    "#             1.0,\n",
    "#             num_timesteps,\n",
    "#             ignore_fraction,\n",
    "#             test_fraction,\n",
    "#         )\n",
    "\n",
    "#     X_train_90, y_train_90, X_test_90, y_test_90 = \\\n",
    "#         read_and_prepare_data(\n",
    "#             path_90,\n",
    "#             2.0,\n",
    "#             num_timesteps,\n",
    "#             ignore_fraction,\n",
    "#             test_fraction,\n",
    "#         )\n",
    "    \n",
    "#     X_train_180, y_train_180, X_test_180, y_test_180 = \\\n",
    "#         read_and_prepare_data(\n",
    "#             path_180,\n",
    "#             3.0,\n",
    "#             num_timesteps,\n",
    "#             ignore_fraction,\n",
    "#             test_fraction,\n",
    "#         )\n",
    "    \n",
    "#     X_train_270, y_train_270, X_test_270, y_test_270 = \\\n",
    "#         read_and_prepare_data(\n",
    "#             path_270,\n",
    "#             4.0,\n",
    "#             num_timesteps,\n",
    "#             ignore_fraction,\n",
    "#             test_fraction,\n",
    "#         )\n",
    "\n",
    "#     X_train = np.concatenate((X_train_stop, X_train_0, X_train_90, X_train_180, X_train_270), axis=0)\n",
    "#     y_train = np.concatenate((y_train_stop, y_train_0, y_train_90, y_train_180, y_train_270), axis=0)\n",
    "#     X_test = np.concatenate((X_test_stop, X_test_0, X_test_90, X_test_180, X_test_270), axis=0)\n",
    "#     y_test = np.concatenate((y_test_stop, y_test_0, y_test_90, y_test_180, y_test_270), axis=0)\n",
    "\n",
    "#     X_train, y_train = shuffle_data(X_train, y_train)\n",
    "#     X_test, y_test = shuffle_data(X_test, y_test)\n",
    "\n",
    "#     return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# path_stop = Path(DATA_PATH, FILENAME_STOP)\n",
    "# path_0 = Path(DATA_PATH, FILENAME_0)\n",
    "# path_90 = Path(DATA_PATH, FILENAME_90)\n",
    "# path_180 = Path(DATA_PATH, FILENAME_180)\n",
    "# path_270 = Path(DATA_PATH, FILENAME_270)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = load_data(\n",
    "#     path_stop,\n",
    "#     path_0,\n",
    "#     path_90,\n",
    "#     path_180,\n",
    "#     path_270,\n",
    "#     NUM_TIMESTEPS,\n",
    "#     IGNORE_FRACTION,\n",
    "#     TEST_FRACTION,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c35712-1089-44f3-89ef-ed471b144c96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99c35712-1089-44f3-89ef-ed471b144c96",
    "outputId": "9076061a-8661-4d2e-c1ed-887a92ec640b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7175, 6, 14), (7175, 1), (1185, 6, 14), (1185, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "VqSuqgBCB_nA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqSuqgBCB_nA",
    "outputId": "26d50bbe-3fd3-4bed-e70e-7003d289c5d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dye8vjENBtXX",
   "metadata": {
    "id": "dye8vjENBtXX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "-3TttkaRCM4l",
   "metadata": {
    "id": "-3TttkaRCM4l"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# CNN = Sequential(name=\"Sequential_CNN\")\n",
    "\n",
    "# CNN.add(Conv2D(256, kernel_size=(3, 3),\n",
    "#                strides=(2, 2), padding=\"same\",\n",
    "#                activation=\"selu\", input_shape=[6, 14, 1]))\n",
    "\n",
    "# CNN.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1),\n",
    "#                      padding=\"valid\"))\n",
    "\n",
    "# # # Add another pair of Conv2D and MaxPooling2D for more model depth,\n",
    "# # # followed by the flatten and multiple dense layers\n",
    "\n",
    "# # CNN.add(Conv2D(64, kernel_size=(3, 3),\n",
    "# #                strides=(2, 2), padding=\"same\",\n",
    "# #                activation=\"selu\"))\n",
    "\n",
    "# # CNN.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n",
    "# #                      padding=\"valid\"))\n",
    "\n",
    "# CNN.add(Flatten())\n",
    "\n",
    "# CNN.add(Dense(64, activation='selu'))\n",
    "# CNN.add(Dense(32, activation='selu'))\n",
    "# CNN.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a85634b-4e9c-45b8-b456-cb970457302f",
   "metadata": {
    "id": "3a85634b-4e9c-45b8-b456-cb970457302f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# CNN = Sequential(name=\"Sequential_CNN\")\n",
    "\n",
    "# CNN.add(Conv2D(256, kernel_size=(3, 3),\n",
    "#                strides=(1, 1), padding=\"same\",\n",
    "#                activation=\"selu\", input_shape=[6, 14, 1]))\n",
    "\n",
    "# CNN.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2),\n",
    "#                      padding=\"valid\"))\n",
    "\n",
    "# # Add another pair of Conv2D and MaxPooling2D for more model depth,\n",
    "# # followed by the flatten and multiple dense layers\n",
    "\n",
    "# # CNN.add(Conv2D(64, kernel_size=(3, 3),\n",
    "# #                strides=(1, 1), padding=\"same\",\n",
    "# #                activation=\"selu\"))\n",
    "\n",
    "# # CNN.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1),\n",
    "# #                      padding=\"valid\"))\n",
    "\n",
    "# CNN.add(Flatten())\n",
    "\n",
    "# CNN.add(Dense(64, activation='selu'))\n",
    "# CNN.add(Dense(32, activation='selu'))\n",
    "# CNN.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bdca018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 6, 14, 256)        2560      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 5, 13, 256)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16640)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                166410    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168,970\n",
      "Trainable params: 168,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 14:49:56.071943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(256, kernel_size=(3, 3),\n",
    "               strides=(1, 1), padding=\"same\",\n",
    "               activation=\"selu\", input_shape=[6, 14, 1]),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1),\n",
    "                     padding=\"valid\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "j0C2RSxICKzq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0C2RSxICKzq",
    "outputId": "af853522-fdf6-40e4-8d0e-cd2a6dcdab9b"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics = ['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911581af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "where_are_NaNsx = isnan(X_train)\n",
    "X_train[where_are_NaNsx] = 0\n",
    "\n",
    "where_are_NaNsy = isnan(y_train)\n",
    "y_train[where_are_NaNsy] = 0\n",
    "\n",
    "where_are_NaNsxt = isnan(X_test)\n",
    "X_test[where_are_NaNsxt] = 0\n",
    "\n",
    "where_are_NaNsyt = isnan(y_test)\n",
    "y_test[where_are_NaNsyt] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7baf8970-5318-4461-b828-d34c232afa5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7baf8970-5318-4461-b828-d34c232afa5f",
    "outputId": "08a66cc8-aeee-42ba-f039-9dfa2ba6f99e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# CNN.compile(\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     optimizer=optimizer,\n",
    "#     metrics=['accuracy'],\n",
    "# )\n",
    "# history = CNN.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=15,\n",
    "#     batch_size=256,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     verbose=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70de8d27-4ff5-4dce-b45f-675dcc74513e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70de8d27-4ff5-4dce-b45f-675dcc74513e",
    "outputId": "7211ef5c-5b4a-4afb-ad4f-7341eeb8f0f8"
   },
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "# _, num_time, num_feat = X_train.shape\n",
    "\n",
    "# activation = 'selu'\n",
    "\n",
    "# DefaultConv1D = partial(\n",
    "#     keras.layers.Conv1D,\n",
    "#     kernel_size=1,\n",
    "#     activation=activation,\n",
    "#     padding=\"valid\",\n",
    "# )\n",
    "\n",
    "# model = keras.models.Sequential([\n",
    "#     DefaultConv1D(4 * num_feat, kernel_size=2, input_shape=[num_time, num_feat]),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     DefaultConv1D(num_feat, kernel_size=2),\n",
    "#     keras.layers.Flatten(),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.Dense(num_time * num_feat, activation=activation),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.Dense(num_feat, activation=activation),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.Dense(5, activation='softmax'),\n",
    "# ])\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# model.compile(\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     optimizer=optimizer,\n",
    "#     metrics=['accuracy'],\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     validation_data=(X_test, y_test),\n",
    "#     verbose=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253f01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc2d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # define the input shape\n",
    "# input_shape = (6, 14)\n",
    "\n",
    "# # create the input placeholder for the model\n",
    "# input_data = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# # reshape input data\n",
    "# input_data_reshaped = tf.keras.layers.Reshape((6, 14, 1))(input_data)\n",
    "\n",
    "# # define the convolutional layer\n",
    "# conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_data_reshaped)\n",
    "\n",
    "# # add maxpooling layer\n",
    "# maxpooling = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(conv1)\n",
    "\n",
    "\n",
    "# reshaped = tf.keras.layers.Reshape((2, 6*32))(maxpooling)\n",
    "\n",
    "# # define the recurrent layer\n",
    "# rnn = tf.keras.layers.GRU(units=64, return_sequences=True)(reshaped)\n",
    "\n",
    "# # add flatten layer\n",
    "# flatten = tf.keras.layers.Flatten()(rnn)\n",
    "\n",
    "# # define the fully connected layers\n",
    "# fc1 = tf.keras.layers.Dense(units=64, activation='relu')(flatten)\n",
    "# fc2 = tf.keras.layers.Dense(units=5, activation='softmax')(fc1)\n",
    "\n",
    "# # define the model\n",
    "# model = tf.keras.Model(inputs=input_data, outputs=fc2)\n",
    "\n",
    "# # compile the model\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f075d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 4s 28ms/step - loss: 0.5337 - sparse_categorical_accuracy: 0.8220 - val_loss: 0.8304 - val_sparse_categorical_accuracy: 0.7420\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 3s 25ms/step - loss: 0.3095 - sparse_categorical_accuracy: 0.8754 - val_loss: 0.7439 - val_sparse_categorical_accuracy: 0.8824\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 3s 25ms/step - loss: 0.1872 - sparse_categorical_accuracy: 0.9409 - val_loss: 0.7222 - val_sparse_categorical_accuracy: 0.9403\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 3s 26ms/step - loss: 0.1187 - sparse_categorical_accuracy: 0.9711 - val_loss: 0.8251 - val_sparse_categorical_accuracy: 0.9109\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 3s 29ms/step - loss: 0.0857 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.8500 - val_sparse_categorical_accuracy: 0.9429\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 3s 26ms/step - loss: 0.0675 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.8255 - val_sparse_categorical_accuracy: 0.9420\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 3s 25ms/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.8946 - val_sparse_categorical_accuracy: 0.9387\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 3s 26ms/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9879 - val_loss: 0.8589 - val_sparse_categorical_accuracy: 0.9454\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 3s 30ms/step - loss: 0.0461 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.8977 - val_sparse_categorical_accuracy: 0.9437\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 3s 30ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.9683 - val_sparse_categorical_accuracy: 0.9403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb38ca03a60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=64, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bed79725-2bb3-47ce-b06c-333c662b30bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "bed79725-2bb3-47ce-b06c-333c662b30bf",
    "outputId": "4354cc75-e44d-4d2a-ac4b-c55a4b3a9305"
   },
   "outputs": [],
   "source": [
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3c0564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history_2.history['sparse_categorical_accuracy'])\n",
    "# plt.plot(history_2.history['val_sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40247d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history_2.history['accuracy'])\n",
    "# plt.plot(history_2.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5841334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b65d81f-3d75-43d1-9e2d-88cef69d110b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "3b65d81f-3d75-43d1-9e2d-88cef69d110b",
    "outputId": "97e18fb9-4340-478c-e307-82c03282e9cc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.plot(history_2.history['loss'])\n",
    "# # plt.plot(history_2.history['val_loss'])\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a964a60-488a-431e-8bcc-586e90d8956b",
   "metadata": {
    "id": "8a964a60-488a-431e-8bcc-586e90d8956b"
   },
   "outputs": [],
   "source": [
    "# y_pred = CNN.predict(X_test)\n",
    "# # y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a89bd1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_model = model.predict(X_test)\n",
    "y_pred_model= np.argmax(y_pred_model, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7C0QyTLHO8q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7C0QyTLHO8q",
    "outputId": "78587010-2aa8-4cc7-e82f-7c40e11ce705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 0.7189 - sparse_categorical_accuracy: 0.9586\n",
      "[0.7188510298728943, 0.9586498141288757]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ljWvGo9skgS",
   "metadata": {
    "id": "1ljWvGo9skgS"
   },
   "outputs": [],
   "source": [
    "# y_pred= np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "BIninuG_Hixu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIninuG_Hixu",
    "outputId": "8d52a3ce-46d5-41c3-f339-ec8e2feee684"
   },
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3NzoyhrD4tO9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NzoyhrD4tO9",
    "outputId": "da09ab77-c439-431e-82a8-93fc98f20b49"
   },
   "outputs": [],
   "source": [
    "# y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48c3e9c7-2bc3-479a-8af1-bf35d87f3e57",
   "metadata": {
    "id": "48c3e9c7-2bc3-479a-8af1-bf35d87f3e57"
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     'y_test': y_test.ravel().astype(int).tolist(),\n",
    "#     'y_pred': y_pred.ravel().astype(int).tolist(),\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d08a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.DataFrame({\n",
    "    'y_test': y_test.ravel().astype(int).tolist(),\n",
    "    'y_pred': y_pred_model.ravel().astype(int).tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18cfafbe-dafb-4a9e-8873-7f96e71f80b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "18cfafbe-dafb-4a9e-8873-7f96e71f80b8",
    "outputId": "aa49cb9d-c88e-437b-9a31-2809577d1da1"
   },
   "outputs": [],
   "source": [
    "# pd.crosstab(df['y_test'], df['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "676f9dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>y_pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>188</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y_pred    0    1    2    3    4\n",
       "y_test                         \n",
       "0       237    0    0    0    0\n",
       "1        17  188    1    0   31\n",
       "2         0    0  237    0    0\n",
       "3         0    0    0  237    0\n",
       "4         0    0    0    0  237"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_model['y_test'], df_model['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e192957b-1b16-47c8-89eb-d6e2fb9bde9f",
   "metadata": {
    "id": "e192957b-1b16-47c8-89eb-d6e2fb9bde9f"
   },
   "outputs": [],
   "source": [
    "model.save('model_f_s_90_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vZD1w6TZwk0e",
   "metadata": {
    "id": "vZD1w6TZwk0e"
   },
   "outputs": [],
   "source": [
    "# CNN.save('model2testar_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413c816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f112bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad8cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e30e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
